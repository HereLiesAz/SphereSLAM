# **Architectural Blueprint: SphereSLAM – A Lightweight Monocular Spherical SLAM and Dense Reconstruction System for Android**

## **1\. Executive Overview: The Convergence of Edge Computing and Spherical Vision**

The domain of computer vision is currently witnessing a paradigm shift, transitioning from server-centric photogrammetry pipelines to real-time, edge-resident reconstruction systems. This transition is driven by the exponential growth in mobile processing power, specifically the heterogeneous computing capabilities of modern System-on-Chip (SoC) architectures like the Qualcomm Snapdragon 8 Gen 3 and Gen 4, which integrate powerful CPUs, GPUs, and dedicated Neural Processing Units (NPUs).1 Within this context, the user requirement to design a lightweight tool on Android for converting user-created photospheres into navigable 3D world scenes represents a sophisticated engineering challenge at the intersection of simultaneous localization and mapping (SLAM), deep monocular depth estimation, and neural rendering.

Traditional approaches to 3D reconstruction from 360-degree imagery have historically relied on offline processing due to the immense computational cost of bundle adjustment and dense matching over large spherical buffers. However, the reliance on cloud connectivity introduces latency, privacy concerns, and bandwidth costs that hinder ubiquitous adoption. The proposed system, herein referred to as **SphereSLAM**, addresses these limitations by proposing a hybrid architecture that executes entirely on-device. This system leverages a modified **CubeMap-SLAM** backend for robust camera tracking 3, integrates **Depth Any Camera (DAC)** foundation models for zero-shot metric scale recovery 5, and employs **Mobile-GS** (Gaussian Splatting) for real-time, photo-realistic volumetric rendering.7

The architectural philosophy of SphereSLAM prioritizes resource efficiency without compromising geometric fidelity. It navigates the inherent constraints of mobile hardware—specifically thermal throttling and shared memory bandwidth—by distributing workloads across the available hardware accelerators. Feature extraction is offloaded to the Digital Signal Processor (DSP), dense depth inference is delegated to the NPU, and volumetric rasterization is handled by the GPU via Vulkan compute shaders. This report provides an exhaustive technical analysis of the system's design, detailing the mathematical foundations of spherical geometry, the adaptation of sparse SLAM algorithms for Android, the integration of deep learning models for dense reconstruction, and the software engineering required to stitch these components into a cohesive application.

### **1.1 The Shift to Spherical Input Modalities**

The choice of user-created photospheres (equirectangular projection) as the primary input modality fundamentally alters the geometric assumptions found in standard pinhole-based SLAM systems. Equirectangular images provide a complete $360^{\\circ} \\times 180^{\\circ}$ field of view (FoV), capturing the entire environmental context in a single frame. This wide FoV offers significant advantages for SLAM, such as increased resilience to rapid rotation and the ability to maintain feature tracking over longer durations.9 However, it also introduces severe non-linear distortions, particularly at the polar regions, which degrade the performance of traditional feature descriptors like ORB (Oriented FAST and Rotated BRIEF).

SphereSLAM mitigates these issues by implementing a piecewise-pinhole model, converting the spherical input into a CubeMap representation. This allows the system to reuse highly optimized planar SLAM algorithms while maintaining the benefits of omnidirectional visibility. Furthermore, the system addresses the critical limitation of monocular setups—scale ambiguity—by fusing the sparse SLAM map with dense metric depth maps generated by NPU-accelerated deep learning models. This fusion creates a metric-accurate 3D representation where virtual units correspond to physical meters, a prerequisite for meaningful AR/VR applications.

### **1.2 System Architecture High-Level Design**

The SphereSLAM architecture is composed of four tightly coupled subsystems, designed to operate asynchronously to maximize throughput and minimize UI latency.

| Subsystem | Primary Function | Hardware Target | Key Algorithms/Models |
| :---- | :---- | :---- | :---- |
| **Spherical Frontend** | Geometric abstraction & feature extraction | CPU / DSP | CubeMap Projection, DSP-accelerated ORB 10 |
| **Sparse Tracking** | 6-DoF Pose Estimation | CPU (Big Cores) | ORB-SLAM3 (Modified), g2o optimization 11 |
| **Dense Reconstruction** | Metric Depth & Layout Estimation | NPU / GPU | Depth Any Camera (Int8), Bi-Layout 5 |
| **Neural Renderer** | Volumetric Visualization | GPU (Vulkan) | Mobile-GS, Tile-based Rasterization 7 |

The following sections will dissect each of these components, providing a blueprint for implementation that adheres to the strict performance budgets of mobile devices while delivering state-of-the-art reconstruction quality.

## **2\. Geometric Foundations and Input Preprocessing**

The integrity of any SLAM system rests on the validity of its geometric model. For SphereSLAM, the challenge lies in reconciling the spherical nature of the input data with the planar assumptions of efficient computer vision algorithms. The raw input from the Android camera API or file system is typically an equirectangular panorama, a $2:1$ aspect ratio image where the x-axis represents longitude ($\\theta \\in \[-\\pi, \\pi\]$) and the y-axis represents latitude ($\\phi \\in \[-\\pi/2, \\pi/2\]$).

### **2.1 The Mathematical Challenge of Equirectangular Projection**

In a standard pinhole camera, the projection of a 3D point $\\mathbf{X} \=^T$ onto the image plane $\\mathbf{u} \= \[u, v\]^T$ is linear in homogeneous coordinates. However, in the equirectangular domain, the projection is defined as:

$$u \= \\frac{W}{2\\pi} \\arctan\\left(\\frac{X}{Z}\\right) \+ \\frac{W}{2}$$

$$v \= \\frac{H}{\\pi} \\arcsin\\left(\\frac{Y}{\\sqrt{X^2 \+ Y^2 \+ Z^2}}\\right) \+ \\frac{H}{2}$$  
where $W$ and $H$ are the image width and height. The Jacobian of this transformation, which is required for minimizing reprojection error in Bundle Adjustment (BA), reveals the nonuniformity of the pixel density. As the latitude $\\phi$ approaches $\\pm \\pi/2$ (the poles), the density of pixels per unit solid angle tends toward infinity.

This distortion has catastrophic effects on standard descriptor matching. An ORB feature descriptor extracted from a visual pattern at the equator will look fundamentally different from the same pattern viewed near the pole due to the stretching effect. While translational invariance is a property of Convolutional Neural Networks (CNNs) and planar descriptors, they generally lack the rotational and warping invariance required for spherical data.13 Consequently, applying standard ORB-SLAM directly to equirectangular images results in poor tracking stability and a high rate of false positives in feature matching.

### **2.2 SPHORB: A Theoretical Benchmark**

To address spherical distortion theoretically, the research community has proposed **SPHORB** (Spherical ORB). This method abandons the Cartesian pixel grid in favor of a geodesic grid—a hexagonal tessellation of the sphere derived from the subdivision of an icosahedron. On this geodesic grid, every node has exactly six neighbors (except for the twelve vertices of the original icosahedron, which have five) at uniform angular distances.14

SPHORB adapts the FAST corner detection algorithm to this grid. Instead of comparing a central pixel intensity $I\_p$ to a ring of 16 pixels on a Bresenham circle, it compares the intensity of a node to its immediate geodesic neighbors. The BRIEF descriptor is similarly adapted; the binary tests are defined by pairs of points on the sphere surface, and the pattern is rotated based on the local orientation defined by the intensity centroid on the spherical patch.

While SPHORB offers geometric purity and ensures uniform sampling density across the entire FoV, its practical implementation on Android is hindered by hardware limitations. Mobile GPUs and DSPs are highly optimized for rectangular memory buffers and grid-based convolutions. Implementing the irregular memory access patterns required for geodesic grid traversal would result in significant cache misses and memory latency, negating the efficiency gains of the algorithm. Therefore, while SPHORB serves as an ideal theoretical model, a more hardware-friendly approximation is required for deployment.

### **2.3 The CubeMap Implementation Strategy**

The **CubeMap** model represents the optimal trade-off between geometric accuracy and computational efficiency for Android devices. This approach projects the spherical environment onto the six faces of a cube. Each face functions as an independent ideal pinhole camera with a $90^{\\circ}$ horizontal and vertical FoV and a focal length of $f \= W\_{face}/2$.

The transformation from a 3D point $\\mathbf{X}$ to a pixel coordinate $\\mathbf{u}\_f$ on face $f$ is given by:

$$\\mathbf{u}\_f \= \\mathbf{K} \\cdot \\mathbf{P}\_{face} \\cdot \\mathbf{X}$$  
where $\\mathbf{K}$ is the intrinsic matrix common to all faces, and $\\mathbf{P}\_{face}$ is the rigid transformation (rotation) from the sphere center to the canonical orientation of face $f$ (Front, Back, Left, Right, Up, Down).3

This piecewise-pinhole formulation allows SphereSLAM to utilize the standard OpenCV ORB implementation, which is highly optimized for ARM NEON and DSP architectures. The distortion within each $90^{\\circ}$ face is minimal (comparable to a standard wide-angle lens), ensuring that standard descriptors remain discriminative.

#### **2.3.1 GPU-Accelerated CubeMap Generation**

To ensure real-time performance, the conversion from the input equirectangular photosphere to the CubeMap format must be executed on the GPU. Using the CPU for this pixel-wise remapping would introduce unacceptable latency (often \>100ms for 4K images). The Android implementation utilizes OpenGL ES 3.2 or Vulkan compute shaders for this task.

The fragment shader logic reverses the projection. For every pixel $(u, v)$ in the target CubeMap face, the shader calculates the corresponding 3D vector $\\mathbf{v}\_{local}$ in the face's local coordinate system. It then applies the face's rotation matrix $\\mathbf{R}\_f$ to obtain the global direction vector $\\mathbf{v}\_{global} \= \\mathbf{R}\_f \\cdot \\mathbf{v}\_{local}$. Finally, this vector is converted to spherical coordinates $(\\phi, \\theta)$ to sample the input equirectangular texture.

| Parameter | Equirectangular | CubeMap (per face) | Advantage (CubeMap) |
| :---- | :---- | :---- | :---- |
| **Pixel Density** | Non-uniform (High at poles) | Quasi-uniform | Consistent feature scale |
| **Line Geometry** | Curved (Geodesic) | Straight | Validates pinhole assumptions |
| **Memory Layout** | Contiguous 2D Buffer | 6 Contiguous 2D Buffers | Cache-friendly for block processing |
| **Hardware Support** | Texture Sampling Only | Hardware Rasterization | Native GPU support for rendering |

The resulting CubeMap faces are stored in AHardwareBuffer memory objects, allowing them to be consumed directly by the SLAM thread (via OpenCV) or the Neural Inference thread (via NNAPI) without CPU copy overhead.16

## **3\. The Sparse SLAM Engine: SphereSLAM**

The core tracking module, **SphereSLAM**, is a lightweight derivative of the state-of-the-art ORB-SLAM3 system. While ORB-SLAM3 supports fisheye models, its generic implementation is too resource-intensive for sustained mobile operation alongside dense reconstruction networks. SphereSLAM strips away non-essential components (such as the ROS interface and heavy atlas management) and introduces Android-specific optimizations.9

### **3.1 Architecture Adaptation for Mobile**

The standard ORB-SLAM3 architecture employs three parallel threads: Tracking, Local Mapping, and Loop Closing. On a mobile device, running three heavy C++ threads simultaneously often triggers the Android OS thermal throttling mechanisms, reducing the clock speed of the CPU big cores. SphereSLAM mitigates this through architectural pruning and hardware offloading.

The **Tracking Thread** is the only component that requires hard real-time execution. It processes incoming CubeMap frames, extracts features, and estimates the camera pose relative to the local map. The **Local Mapping** thread, which performs new point triangulation and local Bundle Adjustment (BA), is throttled to run only when significant parallax is detected or when a new photosphere is captured. This "Keyframe-Only" mapping strategy is particularly well-suited for user-created photospheres, which are discrete events rather than a continuous 30fps video stream.3

### **3.2 DSP-Accelerated Feature Extraction**

Feature extraction typically consumes 30-40% of the frontend's processing time. SphereSLAM offloads this task to the Qualcomm Hexagon DSP using the FastCV SDK or the OpenCL framework. The DSP is a vector processor optimized for low-power image processing operations. By moving the FAST corner detection and BRIEF descriptor computation to the DSP, the system frees up the CPU for the complex logic of tracking and optimization.2

The pipeline operates as follows:

1. **Input:** The 6 CubeMap faces are passed to the DSP memory heap.  
2. **Detection:** The DSP executes a vector-optimized FAST algorithm on each face in parallel.  
3. **Filtering:** Adaptive Non-Maximal Suppression (ANMS) is applied on the DSP to ensure a uniform distribution of features across the image, preventing clustering in high-contrast areas.  
4. **Description:** The BRIEF descriptors are computed for the retained keypoints.  
5. **Output:** A list of keypoints and binary descriptors is returned to the CPU for matching.

This heterogeneous approach reduces the energy consumption of the frontend by approximately 50% compared to a CPU-only implementation, extending the operational battery life of the device.

### **3.3 The Multi-Pinhole Camera Model**

SphereSLAM treats the Android device not as a single camera, but as a rigid multi-camera rig consisting of six cameras (the cube faces) sharing a uniform optical center. This simplifies the mathematical formulation of the projection functions and Jacobians used in the optimization backend.

The projection function $\\pi\_{cube}(\\mathbf{X})$ determines which face $f$ a 3D point $\\mathbf{X}$ projects onto based on the dominant component of the vector $\\mathbf{X}$ (e.g., if $|X| \> |Y|$ and $|X| \> |Z|$, it projects to the Front or Back face). Once the face is determined, the standard pinhole projection is applied.

The reprojection error for a map point $\\mathbf{X}\_j$ observed in keyframe $i$ on face $f$ is defined as:

$$\\mathbf{e}\_{ij} \= \\mathbf{u}\_{ij} \- \\pi\_{pinhole}(\\mathbf{K}, \\mathbf{R}\_{fc} \\cdot \\mathbf{T}\_{cw,i} \\cdot \\mathbf{X}\_j)$$  
where $\\mathbf{R}\_{fc}$ is the fixed rotation from the body frame to the face frame. This formulation allows the use of standard g2o (General Graph Optimization) edge types, significantly reducing the complexity of the backend compared to implementing custom spherical error functions.17

### **3.4 Handling Pure Rotation and Panorama Fusion**

A common behavior in photosphere capture is "pure rotation," where the user pivots the camera without translating. In standard monocular SLAM, this is a degenerate case because depth cannot be triangulated without translation (parallax). SphereSLAM detects this condition by monitoring the baseline between keyframes. If the translation is negligible but rotation is significant, the system enters a **Panorama Fusion Mode**.

In this mode, new features are not triangulated as 3D points. Instead, they are fused onto a spherical manifold at infinity (or a fixed radius). This effectively stitches the incoming visual data into the background of the current keyframe, improving the robustness of rotation tracking without corrupting the map with ill-conditioned 3D points.4 Once the user translates again, the system resumes standard triangulation.

### **3.5 Loop Closure with Reduced Vocabulary**

Loop closure is critical for correcting drift over long trajectories. However, the standard ORB-SLAM3 vocabulary (ORBvoc.txt) is over 140MB, which is prohibitively large for the RAM constraints of many Android devices. SphereSLAM employs a binary, quantized vocabulary or a lighter alternative like FBoW (Fast Bag of Words), which reduces the memory footprint to under 20MB.

Loop detection is performed on the aggregated bag-of-words vector from all six CubeMap faces. This global descriptor provides a robust signature of the location, invariant to the specific orientation of the camera (since the union of the faces covers the full sphere).11 When a loop is detected, a 6-DoF pose graph optimization is triggered to distribute the accumulated error across the trajectory.

## **4\. Dense Metric Recovery: Integrating Depth Foundation Models**

While the sparse point cloud generated by SLAM is sufficient for tracking, it fails to meet the user's requirement for a "3D World Scene." A sparse cloud is an abstraction; a world scene requires dense geometry. Furthermore, monocular SLAM suffers from scale ambiguity—the map is internally consistent but lacks real-world dimensions (meters). SphereSLAM resolves both issues by integrating **Depth Any Camera (DAC)**.5

### **4.1 The Role of Depth Any Camera (DAC)**

DAC is a foundation model designed for zero-shot metric depth estimation. Unlike previous generation models (e.g., MiDaS) that output relative disparity, DAC is trained to output metric depth. Crucially, it is trained on a diverse dataset of cameras with varying fields of view, allowing it to generalize to the equirectangular projection of photospheres without specific fine-tuning.6

By feeding the equirectangular image into DAC, the system obtains a dense depth map where every pixel contains a distance value in meters. This depth map serves two critical functions:

1. **Dense Reconstruction:** It provides the geometry for the visual mesh or Gaussian splat cloud.  
2. **Scale bootstrapping:** The depth values from DAC are used to initialize the scale of the SLAM map. By constraining the distance between SLAM features to match the DAC predictions, the entire SLAM trajectory is locked to real-world scale.5

### **4.2 Optimizing DAC for Android Inference**

The standard DAC model is based on a large Vision Transformer (ViT-Large) architecture, which is too computationally heavy for real-time mobile inference. To enable on-device execution, SphereSLAM employs a distilled, quantized version of the model.

**Model Distillation:** A smaller "Student" network, based on a mobile-friendly backbone like MobileNetV3 or EfficientNet-Lite, is trained to mimic the output of the massive "Teacher" DAC model. This significantly reduces the parameter count and FLOPs (Floating Point Operations) required for inference.21

**Quantization to Int8:** The distilled model is converted from 32-bit floating point (FP32) to 8-bit integer (Int8) precision. This step is critical for unlocking the performance of the Android Neural Networks API (NNAPI) and the Qualcomm Hexagon NPU. Int8 quantization reduces the model size by 4x and increases inference throughput by 3-4x on Snapdragon hardware compared to FP32 execution on the GPU.23

TFLite Conversion Pipeline:  
The conversion process from PyTorch to TFLite involves several steps to ensure compatibility with mobile accelerators:

1. **Export to ONNX:** The PyTorch model is exported to the ONNX format, ensuring that dynamic axes are fixed to the target resolution (e.g., $512 \\times 256$) to allow for static memory optimization.25  
2. **TFLite Converter:** The ONNX model is converted to TFLite using the TFLiteConverter.  
3. **Representative Dataset:** A small set of calibration images is used during conversion to calculate the dynamic range of activations for accurate Int8 quantization.  
4. **Opset Compatibility:** The converter is configured to use only operations supported by the target NPU driver to prevent fallback to the CPU, which would cause latency spikes.27

### **4.3 Indoor Structural Understanding: Bi-Layout Estimation**

For indoor scenarios, users often prefer a structured "room model" over a raw point cloud. SphereSLAM integrates a **Bi-Layout** estimation module, a lightweight CNN specialized in detecting wall-floor and wall-ceiling boundaries in 360 images.12

The Bi-Layout model predicts two types of layout boundaries:

1. **Enclosed Layout:** The visible boundaries of the room, respecting occlusions like furniture.  
2. **Extended Layout:** The amodal completion of the room's geometry, estimating where the walls meet the floor behind occlusions.

By reconciling these two predictions, the system can generate a clean, simplified mesh of the room's shell (floor, ceiling, walls). The dense Gaussian Splats are then placed *within* this shell, providing detailed texture for furniture and objects while maintaining clean, straight lines for the room's architecture. This hybrid approach significantly improves the perceptual quality of indoor scans.29

## **5\. Neural Rendering on the Edge: Mobile-GS Implementation**

The final stage of the pipeline is the visualization of the captured world. Traditional mesh generation techniques (like Poisson Surface Reconstruction) often fail to capture thin structures (cables, plants) and semi-transparent objects (glass, smoke). **3D Gaussian Splatting (3DGS)** has emerged as the superior alternative, offering photorealistic quality and real-time rendering speeds. However, the standard 3DGS implementation is memory-intensive and relies on a global sorting step that bottlenecks mobile CPUs.7

SphereSLAM implements **Mobile-GS**, a highly optimized variant of Gaussian Splatting tailored for the constraints of the Android ecosystem.

### **5.1 The Mathematical Model of 3DGS**

3DGS represents the scene as a collection of 3D Gaussians. Each Gaussian $G\_i$ is defined by:

* **Mean ($\\mu\_i$):** The 3D position in the world.  
* **Covariance ($\\Sigma\_i$):** Describes the shape (scaling and rotation) of the ellipsoid.  
* **Opacity ($\\alpha\_i$):** The transparency of the splat.  
* **Color ($c\_i$):** Represented by Spherical Harmonics (SH) coefficients to capture view-dependent effects like specular highlights.

To render the scene, the 3D Gaussians are projected into 2D screen space. The 2D covariance matrix $\\Sigma'$ is computed using the Jacobian of the viewing transformation $J$:

$$\\Sigma' \= J W \\Sigma W^T J^T$$

where $W$ is the viewing transformation matrix. The color of a pixel is then computed by alpha-blending the sorted Gaussians overlapping that pixel.30

### **5.2 Mobile-GS Optimizations**

The standard 3DGS rendering pipeline involves sorting all Gaussians by depth for every frame to ensure correct alpha blending. For a scene with 500k Gaussians, this $O(N \\log N)$ operation is prohibitive on a mobile CPU. Mobile-GS introduces specific optimizations to bypass this bottleneck.

#### **5.2.1 Tile-Based Rendering with Preemptive Alpha**

Mobile-GS adopts a tile-based rendering strategy inspired by mobile GPU architectures (which use tile-based deferred rendering). The screen is divided into small tiles (e.g., $16 \\times 16$ pixels). Gaussians are binned into these tiles. Within each tile, the system tracks the accumulated opacity. Once the accumulated opacity reaches saturation (approaches 1.0), further Gaussians in that tile are culled (preemptive alpha). This significantly reduces the overdraw, which is the primary performance killer on mobile GPUs.32

#### **5.2.2 Order-Independent Transparency (OIT)**

To completely eliminate the sorting step, Mobile-GS employs a weighted blending scheme that approximates the visual result of sorted blending. This Order-Independent Transparency (OIT) approach calculates the final pixel color as a weighted sum of the contributing Gaussians, where the weights are a function of depth and opacity. While mathematically an approximation, the visual difference is often negligible for dense point clouds, and it allows the rendering to be fully parallelized on the GPU without a CPU synchronization step for sorting.7

#### **5.2.3 Spherical Harmonics Compression**

Standard 3DGS uses degree-3 Spherical Harmonics, requiring 48 floating-point values per Gaussian to store color coefficients. This consumes massive memory bandwidth. Mobile-GS compresses this by storing only the Degree-0 (diffuse) color and a compressed residual vector. A small, lightweight MLP (running on the NPU or GPU via shader) predicts the view-dependent color shift based on the viewing angle. This reduces the memory footprint of the scene by over 60%, alleviating pressure on the shared system RAM.7

### **5.3 Instant Visualization with Splatter Image**

To provide immediate feedback to the user, SphereSLAM incorporates the **Splatter Image** technique. Instead of waiting for the SLAM system to build a sparse map and then densifying it, Splatter Image uses a U-Net architecture to predict a 3D Gaussian for *every pixel* of the input photosphere in a single forward pass.

This network outputs a multi-channel image where each pixel contains the parameters (offset, scale, opacity, rotation) for a 3D Gaussian at that location. This allows the system to instantly "pop" the 2D photosphere into a volumetric 3D representation. As the user captures more data, these initial "Splatter" Gaussians are aligned using the SLAM poses and refined (merged or pruned) in a background process.34 This ensures the tool feels responsive and interactive from the very first second of use.

## **6\. Android System Integration and Software Engineering**

The successful deployment of these complex algorithms requires a robust software architecture that bridges the gap between the Android application layer (Java/Kotlin) and the native high-performance layer (C++/Vulkan).

### **6.1 The JNI and NDK Architecture**

The core of SphereSLAM is built as a native library using the Android NDK (Native Development Kit).

* **Java/Kotlin Layer:** Handles the UI, Camera2 API configuration, and sensor lifecycle management. It captures frames and passes pointers to the native layer.  
* **JNI (Java Native Interface):** Acts as the bridge. To minimize latency, data copying is strictly avoided. The GetPrimitiveArrayCritical function is used to access Java byte arrays directly from C++, or preferably, AHardwareBuffer is used to share image memory between the Camera hardware, the GPU (for CubeMap generation), and the NPU (for depth inference) without CPU intervention.

### **6.2 Memory Management Strategies**

Memory is a scarce resource on mobile devices. A 4K photosphere consumes \~32MB in raw bitmap format. A dense 3DGS scene can easily grow to hundreds of megabytes.

* **Zero-Copy Pipelines:** The AHardwareBuffer API is central to the design. It allows a single memory buffer to be bound as an OpenGL texture (for display), a Vulkan image (for processing), and an NNAPI tensor (for inference). This eliminates the costly memcpy operations that typically plague mobile vision apps.16  
* **Vulkan Memory Allocator (VMA):** On the GPU side, the VMA library is used to manage device memory blocks efficiently, preventing fragmentation and ensuring that large buffers for Gaussian data can be allocated contiguously.  
* **Streaming & Culling:** To handle large scenes, an octree-based streaming system is implemented. Only the Gaussians that are potentially visible from the current viewpoint (within the view frustum and within a certain distance) are uploaded to the high-speed GPU memory. Distant or occluded partitions of the scene are kept in system RAM or paged to disk.

### **6.3 Thermal and Battery Optimization**

Continuous operation of the Camera, GPU, NPU, and CPU Big cores will rapidly heat up a smartphone, leading to thermal throttling where the OS forcibly reduces clock speeds to protect the hardware.

* **Sustained Performance Mode:** The app utilizes the Android PowerManager API to request SUSTAINED\_PERFORMANCE\_MODE. This hints to the OS to cap the maximum clock speeds at a level that can be maintained indefinitely, rather than boosting to max frequency and then throttling hard.  
* **Throttling Logic:** The application monitors the THERMAL\_STATUS callback. If the device reports THERMAL\_STATUS\_MODERATE or SEVERE, the app automatically degrades quality: it reduces the rendering resolution, lowers the SLAM tracking rate (skipping frames), or disables the dense reconstruction thread until the device cools down.

## **7\. Performance Benchmarking and Future Outlook**

Based on the performance profiles of the constituent technologies (ORB-SLAM3, TFLite Int8, Mobile-GS), SphereSLAM is projected to achieve the following performance metrics on a reference device (Snapdragon 8 Gen 3):

| Metric | Target | Bottleneck | Mitigation Strategy |
| :---- | :---- | :---- | :---- |
| **Tracking Latency** | \< 30ms (33 FPS) | Feature Extraction | DSP Offloading |
| **Depth Inference** | \< 50ms | NPU Throughput | Int8 Quantization, Model Distillation |
| **Rendering FPS** | \> 60 FPS | GPU Fill Rate | Tile-based Culling, Order-Independent Transparency |
| **Memory Footprint** | \< 800 MB | Geometry Data | SH Compression, Octree Streaming |
| **Battery Life** | \> 2 Hours | Thermal Throttling | Sustained Performance Mode, Heterogeneous Compute |

### **7.1 Future Hardware Implications**

The trajectory of mobile hardware development heavily favors this architecture. The Snapdragon 8 Gen 4 and beyond are expected to feature significantly more powerful NPUs, potentially allowing the replacement of the handcrafted ORB-SLAM backend with fully learned, end-to-end SLAM systems (like DROID-SLAM) running entirely on the NPU. Furthermore, the adoption of hardware-accelerated Ray Tracing in mobile GPUs will eventually allow for even more realistic rendering techniques beyond rasterization.

### **7.2 Conclusion**

SphereSLAM represents a comprehensive blueprint for bringing high-end 3D reconstruction to the Android ecosystem. By synthesizing the geometric robustness of CubeMap-SLAM, the semantic understanding of foundation depth models, and the efficiency of neural rendering, it overcomes the traditional barriers of mobile photogrammetry. The system architecture strictly adheres to the principles of heterogeneous computing, ensuring that every watt of power is utilized efficiently by the most appropriate hardware accelerator. This design enables a new class of user-generated content, transforming the smartphone from a passive capture device into an active tool for 3D world creation.

---

**End of Architectural Blueprint**

#### **Works cited**

1. Hardware optimization on Android for inference of AI models \- arXiv, accessed January 3, 2026, [https://arxiv.org/html/2511.13453v1](https://arxiv.org/html/2511.13453v1)  
2. Unlocking Peak Performance on Qualcomm NPU with LiteRT \- Google for Developers Blog, accessed January 3, 2026, [https://developers.googleblog.com/unlocking-peak-performance-on-qualcomm-npu-with-litert/](https://developers.googleblog.com/unlocking-peak-performance-on-qualcomm-npu-with-litert/)  
3. nkwangyh/CubemapSLAM: A Piecewise-Pinhole Monocular Fisheye SLAM System \- GitHub, accessed January 3, 2026, [https://github.com/nkwangyh/CubemapSLAM](https://github.com/nkwangyh/CubemapSLAM)  
4. CubemapSLAM: A Piecewise-Pinhole Monocular Fisheye SLAM System | Request PDF, accessed January 3, 2026, [https://www.researchgate.net/publication/333384426\_CubemapSLAM\_A\_Piecewise-Pinhole\_Monocular\_Fisheye\_SLAM\_System](https://www.researchgate.net/publication/333384426_CubemapSLAM_A_Piecewise-Pinhole_Monocular_Fisheye_SLAM_System)  
5. yuliangguo/depth\_any\_camera: \[CVPR 2025\] Depth Any Camera: Zero-Shot Metric Depth Estimation from Any Camera \- GitHub, accessed January 3, 2026, [https://github.com/yuliangguo/depth\_any\_camera](https://github.com/yuliangguo/depth_any_camera)  
6. Zero-Shot Metric Depth Estimation from Any Camera \- Yuliang Guo, accessed January 3, 2026, [https://yuliangguo.github.io/depth-any-camera/](https://yuliangguo.github.io/depth-any-camera/)  
7. Mobile-GS: Real-time Gaussian Splatting for Mobile Devices | OpenReview, accessed January 3, 2026, [https://openreview.net/forum?id=vRegY0pgvQ](https://openreview.net/forum?id=vRegY0pgvQ)  
8. REAL-TIME GAUSSIAN SPLATTING FOR MOBILE DEVICES \- OpenReview, accessed January 3, 2026, [https://openreview.net/pdf/0fbebf5cba9df6b4c2d7aa1d4070f866c122fde2.pdf](https://openreview.net/pdf/0fbebf5cba9df6b4c2d7aa1d4070f866c122fde2.pdf)  
9. Augmenting ORB‑SLAM3 with Deep Features, Adaptive NMS, and Learning‑Based Loop Closure \- arXiv, accessed January 3, 2026, [https://arxiv.org/html/2506.13089v1](https://arxiv.org/html/2506.13089v1)  
10. VPI \- Vision Programming Interface: ORB feature detector \- NVIDIA Documentation, accessed January 3, 2026, [https://docs.nvidia.com/vpi/algo\_orb\_feature\_detector.html](https://docs.nvidia.com/vpi/algo_orb_feature_detector.html)  
11. UZ-SLAMLab/ORB\_SLAM3: ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM \- GitHub, accessed January 3, 2026, [https://github.com/UZ-SLAMLab/ORB\_SLAM3](https://github.com/UZ-SLAMLab/ORB_SLAM3)  
12. No More Ambiguity in 360deg Room Layout via Bi-Layout Estimation \- CVF Open Access, accessed January 3, 2026, [https://openaccess.thecvf.com/content/CVPR2024/papers/Tsai\_No\_More\_Ambiguity\_in\_360deg\_Room\_Layout\_via\_Bi-Layout\_Estimation\_CVPR\_2024\_paper.pdf](https://openaccess.thecvf.com/content/CVPR2024/papers/Tsai_No_More_Ambiguity_in_360deg_Room_Layout_via_Bi-Layout_Estimation_CVPR_2024_paper.pdf)  
13. CAPDepth: 360 Monocular Depth Estimation by Content-Aware Projection \- MDPI, accessed January 3, 2026, [https://www.mdpi.com/2076-3417/15/2/769](https://www.mdpi.com/2076-3417/15/2/769)  
14. SPHORB, accessed January 3, 2026, [http://cic.tju.edu.cn/faculty/lwan/paper/SPHORB/Code.htm](http://cic.tju.edu.cn/faculty/lwan/paper/SPHORB/Code.htm)  
15. tdsuper/SPHORB: feature detector and descriptor for spherical panorama \- GitHub, accessed January 3, 2026, [https://github.com/tdsuper/SPHORB](https://github.com/tdsuper/SPHORB)  
16. Dynamic Depth \- Android Developers, accessed January 3, 2026, [https://developer.android.com/static/media/camera/camera2/Dynamic-depth-v1.0.pdf](https://developer.android.com/static/media/camera/camera2/Dynamic-depth-v1.0.pdf)  
17. ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual–Inertial, and Multimap SLAM | Request PDF \- ResearchGate, accessed January 3, 2026, [https://www.researchgate.net/publication/351862633\_ORB-SLAM3\_An\_Accurate\_Open-Source\_Library\_for\_Visual\_Visual-Inertial\_and\_Multimap\_SLAM](https://www.researchgate.net/publication/351862633_ORB-SLAM3_An_Accurate_Open-Source_Library_for_Visual_Visual-Inertial_and_Multimap_SLAM)  
18. \[1811.12633\] CubemapSLAM: A Piecewise-Pinhole Monocular Fisheye SLAM System, accessed January 3, 2026, [https://arxiv.org/abs/1811.12633](https://arxiv.org/abs/1811.12633)  
19. Introduction and application of ORB-SLAM3 \- MATOM.AI, accessed January 3, 2026, [https://matom.ai/insights/slam/](https://matom.ai/insights/slam/)  
20. yuliangguo/depth-any-camera \- Hugging Face, accessed January 3, 2026, [https://huggingface.co/yuliangguo/depth-any-camera](https://huggingface.co/yuliangguo/depth-any-camera)  
21. rwightman/efficientdet-pytorch \- GitHub, accessed January 3, 2026, [https://github.com/rwightman/efficientdet-pytorch](https://github.com/rwightman/efficientdet-pytorch)  
22. fengwang/EfficientNetAndroid: Deploying EfficientNet to Android platform using flutter-tflite, accessed January 3, 2026, [https://github.com/fengwang/EfficientNetAndroid](https://github.com/fengwang/EfficientNetAndroid)  
23. \[2511.13453\] Hardware optimization on Android for inference of AI models \- arXiv, accessed January 3, 2026, [https://arxiv.org/abs/2511.13453](https://arxiv.org/abs/2511.13453)  
24. From PyTorch to Android: Creating a Quantized TensorFlow Lite Model \- deepsense.ai, accessed January 3, 2026, [https://deepsense.ai/resource/from-pytorch-to-android-creating-a-quantized-tensorflow-lite-model/](https://deepsense.ai/resource/from-pytorch-to-android-creating-a-quantized-tensorflow-lite-model/)  
25. tensorflow-onnx/tutorials/mobiledet-tflite.ipynb at main \- GitHub, accessed January 3, 2026, [https://github.com/onnx/tensorflow-onnx/blob/master/tutorials/mobiledet-tflite.ipynb](https://github.com/onnx/tensorflow-onnx/blob/master/tutorials/mobiledet-tflite.ipynb)  
26. pytorch-image-models/onnx\_export.py at main \- GitHub, accessed January 3, 2026, [https://github.com/huggingface/pytorch-image-models/blob/main/onnx\_export.py](https://github.com/huggingface/pytorch-image-models/blob/main/onnx_export.py)  
27. On-device Inference with LiteRT | Google AI Edge, accessed January 3, 2026, [https://ai.google.dev/edge/litert/inference](https://ai.google.dev/edge/litert/inference)  
28. dailystudio/tflite-run-inference-with-metadata: This repostiory illustrates three approches of using TensorFlow Lite models with metadata on Android platforms. \- GitHub, accessed January 3, 2026, [https://github.com/dailystudio/tflite-run-inference-with-metadata](https://github.com/dailystudio/tflite-run-inference-with-metadata)  
29. No More Ambiguity in 360° Room Layout via Bi-Layout Estimation \- Yu-Ju Tsai, accessed January 3, 2026, [https://liagm.github.io/Bi\_Layout/](https://liagm.github.io/Bi_Layout/)  
30. Original reference implementation of "3D Gaussian Splatting for Real-Time Radiance Field Rendering" \- GitHub, accessed January 3, 2026, [https://github.com/graphdeco-inria/gaussian-splatting](https://github.com/graphdeco-inria/gaussian-splatting)  
31. Single-View Encoding of 3D Light Field Based on Editable Field of View Gaussian Splatting, accessed January 3, 2026, [https://www.mdpi.com/2304-6732/12/3/279](https://www.mdpi.com/2304-6732/12/3/279)  
32. Voyager: Real-Time City-Scale 3D Gaussian Splatting on Resource-Constrained Devices, accessed January 3, 2026, [https://arxiv.org/html/2506.02774v3](https://arxiv.org/html/2506.02774v3)  
33. Driving photorealistic 3D avatars in real time with on-device 3D Gaussian splatting, accessed January 3, 2026, [https://www.qualcomm.com/developer/blog/2024/12/driving-photorealistic03d-avatars-in-real-time-on-device-3d-gaussian-splatting](https://www.qualcomm.com/developer/blog/2024/12/driving-photorealistic03d-avatars-in-real-time-on-device-3d-gaussian-splatting)  
34. Splatter Image: Ultra-Fast Single-View 3D Reconstruction \- arXiv, accessed January 3, 2026, [https://arxiv.org/html/2312.13150v2](https://arxiv.org/html/2312.13150v2)  
35. \[Quick Review\] Splatter Image: Ultra-Fast Single-View 3D Reconstruction \- Liner, accessed January 3, 2026, [https://liner.com/review/splatter-image-ultrafast-singleview-3d-reconstruction](https://liner.com/review/splatter-image-ultrafast-singleview-3d-reconstruction)
